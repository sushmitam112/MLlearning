{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025515fa-b068-47cc-8f2d-9b0258752650",
   "metadata": {},
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f357e33-e11d-4494-bb93-19b39cf5cdb8",
   "metadata": {},
   "source": [
    "* Classic problem of ML - classifying handwritten/pics of numbers into what the number represents\n",
    "  * the last layer has 10 neurons each representing one of the digits\n",
    "  * the brightest one/one with highest number inside is the \n",
    "* Each neuron can be thought of as a function/weighted sum of all the previous layer nodes + bias\n",
    "* The bias is used in case it has to be over a certain limit for instance if want to activate > 10 put -10 at the end\n",
    "* Learning - finding right weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47045c72-d829-4ed3-a2f0-ace24ba4cfc8",
   "metadata": {},
   "source": [
    "# Using Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d89f5-bd2f-402f-89c6-85977318de77",
   "metadata": {},
   "source": [
    "* Training data -  a bunch of digit pictures and the corresponding digit it is which is used to \"train\" the model\n",
    "  * to test pass in new data the model hasnt seen and make it predict the corresponding digit\n",
    "  * that means the neuron wich is activated the most in the last layer - highest number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb4f51-47f1-4165-aed5-5e28723d0c23",
   "metadata": {},
   "source": [
    "# Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e5c76-2811-4308-88b1-53db9e893b87",
   "metadata": {},
   "source": [
    "* the weights are like the strenghts of the connections between the neuron and the one it is connected to in the previous layer\n",
    "* Cost function - way of telling computer to quantify how \"wrong\" a certain output is\n",
    "  * add up squares of difference between expected and actual value -> cost of training example\n",
    "  * smaller when the model is correct\n",
    "  * then use average cost across all the training examples\n",
    "* input - the thousands of wieghts/biases of all the connections/neurons\n",
    "* output - 1 number - which is the average cost\n",
    "* parameters - lots of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd765e1-d581-4858-8307-dd59127b6b1e",
   "metadata": {},
   "source": [
    "* how does it change its weights/biases based on this cost function?\n",
    "* want to minimize this cost function - use calculus to figure out which direction to \"move\" so that the cost function goes down\n",
    "  * step size based on slope helps to get more exact at the bottom/closer to the min\n",
    "  * only local minimum, global minimum is much harder to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1803175a-9f35-4650-ac1d-37d26441be0a",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad017d-21f7-4ed8-9cf0-efa564ae6af2",
   "metadata": {},
   "source": [
    "* if there are 2 inputs then its a 3d graph\n",
    "  * which direction to step to decrease the funciton most quickly\n",
    "  * this can be represented as a vector -> way to calculate this vector using multivariable calculus\n",
    "* then you can use this gradient vector to change all the weights in a way which would decrease the cost function the most\n",
    "  * this is why the neurons have a range of activations instead of just being 0 and 1\n",
    "* gradient descent - repreatedly nudging weights/imputs by multiple of negative gradient\n",
    "* there are some changes in the gradient descent which will make more of a difference\n",
    "* the ones with higher gradient numbers carry more \"bang for their buck\" in getting to the minimum\n",
    "  * \"which changes to which weights matter the most\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7baefe-1706-485c-aac7-89a8008db727",
   "metadata": {},
   "source": [
    "# Analyzing the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5562f4-c4a4-4260-97ad-fceb2a6b736f",
   "metadata": {},
   "source": [
    "* the gradient descent is pretty good and has a 96% accuracy when initalized with random numbers\n",
    "* each layer doesn't do a specific task though e.g. finding edges\n",
    "* it isnt able to differentiate between random noise picture and is able to \"find\" digits\n",
    "  * it somehow found a pattern to mostly accurate classify real digits - but can't actually draw one\n",
    "* because it is trained to have such confidence in what each digit in the given pic is supposed to be it remains that way even for random pictures\n",
    "* structed datasts lead to much faster learning/being able to find local minima better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9fe22-4642-43a1-9730-da5f24c30fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
